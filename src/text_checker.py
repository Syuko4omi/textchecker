import re
import argparse
import streamlit as st
from annotated_text import annotated_text
import sqlite3
import gensim

from module_length import length_funcs, proofread_with_chatgpt
from module_wordy import wordy_funcs, tautological_funcs
from module_appearance import appearance_funcs
from module_expression import overused_funcs, preparation, get_synonym
from module_expression.config import POS_LIST


def create_layout():
    st.set_page_config(layout="wide")  # ãƒšãƒ¼ã‚¸ã®æ¨ªå¹…ã‚’ãƒ•ãƒ«ã«ä½¿ã†
    uploaded_file = st.sidebar.file_uploader("ğŸ“ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«", accept_multiple_files=False)

    show_element = st.sidebar.selectbox(
        "âš™ï¸è¡¨ç¤ºã™ã‚‹è¦ç´ ",
        [
            "é•·ã™ãã‚‹æ–‡",
            "èª­ç‚¹ãŒå¤šã„æ–‡",
            "èª­ç‚¹ãŒãªã„æ–‡",
            "å†—é•·ãªè¡¨ç¾",
            "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾",
            "åŠè§’ãƒ»å…¨è§’",
            "ä½¿ã‚ã‚Œã™ããªè¡¨ç¾",
        ],
    )

    hankaku_zenkaku = st.sidebar.selectbox(
        "ğŸ”¤è‹±æ•°å­—ãƒ»ã‚«ãƒŠãƒ»è¨˜å·ï¼ˆã€ŒåŠè§’ãƒ»å…¨è§’ã€ã‚’é¸ã‚“ã æ™‚ã®ã¿æœ‰åŠ¹ï¼‰",
        ["åŠè§’è‹±æ•°å­—", "å…¨è§’è‹±æ•°å­—", "åŠè§’ã‚«ã‚¿ã‚«ãƒŠ", "åŠè§’è¨˜å·"],
    )

    selected_items = st.sidebar.multiselect(
        "ğŸ’¬å“è©ï¼ˆã€Œä½¿ã‚ã‚Œã™ããªè¡¨ç¾ã€ã‚’é¸ã‚“ã æ™‚ã®ã¿æœ‰åŠ¹ãƒ»è¤‡æ•°é¸æŠå¯ï¼‰",
        POS_LIST,
    )

    return uploaded_file, show_element, hankaku_zenkaku, selected_items


def prepare_tools_for_analysis():
    wordy_expression_dict = wordy_funcs.create_wordy_expression_dict()
    tautological_expression_dict = (
        tautological_funcs.create_tautological_expression_dict()
    )
    tokenizer = preparation.prepare_tokenizer()
    return wordy_expression_dict, tautological_expression_dict, tokenizer


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--file_name", default="./data/demo.txt", help="put your text file name"
    )
    args = parser.parse_args()
    return args


def wrapper_function(
    show_element,
    sentences,
    row_num,
    correspondence_dict,
    hankaku_zenkaku,
    tokenizer,
    overused_parts,
    problematic_level_dict,
):
    element_to_func = {
        "é•·ã™ãã‚‹æ–‡": length_funcs.lengthy_checker,
        "èª­ç‚¹ãŒå¤šã„æ–‡": length_funcs.punctuation_num_checker,
        "èª­ç‚¹ãŒãªã„æ–‡": length_funcs.continuous_checker,
        "å†—é•·ãªè¡¨ç¾": wordy_funcs.wordy_expression_checker,
        "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾": tautological_funcs.tautological_expression_checker,
        "åŠè§’ãƒ»å…¨è§’": appearance_funcs.appearance_checker,
        "ä½¿ã‚ã‚Œã™ããªè¡¨ç¾": overused_funcs.overused_expression_checker,
    }
    if show_element in ["é•·ã™ãã‚‹æ–‡", "èª­ç‚¹ãŒå¤šã„æ–‡", "èª­ç‚¹ãŒãªã„æ–‡"]:
        annotated_text_list, text_position_list, advice_list = element_to_func[
            show_element
        ](sentences, row_num)
    elif show_element in ["å†—é•·ãªè¡¨ç¾", "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾"]:
        annotated_text_list, text_position_list, advice_list = element_to_func[
            show_element
        ](sentences, correspondence_dict[show_element], row_num)
    elif show_element == "åŠè§’ãƒ»å…¨è§’":
        annotated_text_list, text_position_list, advice_list = element_to_func[
            show_element
        ](sentences, row_num, hankaku_zenkaku)
    else:
        (
            annotated_text_list,
            text_position_list,
            advice_list,
        ) = element_to_func[
            show_element
        ](sentences, row_num, tokenizer, overused_parts, problematic_level_dict)
    return annotated_text_list, text_position_list, advice_list


if __name__ == "__main__":
    my_args = get_args()
    uploaded_file, show_element, hankaku_zenkaku, selected_items = create_layout()

    if selected_items == []:
        selected_items = POS_LIST

    annotated_texts = []  # ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç¾¤
    pos_list = []  # æŒ‡æ‘˜ã—ãŸæ–‡ç« ã®å ´æ‰€
    advices_list = []  # æŒ‡æ‘˜ã®å…·ä½“çš„ãªå†…å®¹
    (
        wordy_expression_dict,
        tautological_expression_dict,
        tokenizer,
    ) = prepare_tools_for_analysis()
    f_r = open(my_args.file_name, "r")
    text_lists = (
        uploaded_file.read().decode("utf-8").splitlines()
        if uploaded_file is not None
        else f_r.read().splitlines()
    )
    f_r.close()
    overused_parts = overused_funcs.find_overused_part(  # é »ç™ºã™ã‚‹è¡¨ç¾ä¸Šä½20ä»¶ã®ãƒªã‚¹ãƒˆ
        text_lists, tokenizer, pos_option=selected_items
    )
    problematic_level_dict = overused_funcs.overused_level_indicator(overused_parts)
    correspondence_dict = {
        "å†—é•·ãªè¡¨ç¾": wordy_expression_dict,
        "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾": tautological_expression_dict,
    }
    for row_num, text in enumerate(text_lists):  # æ”¹è¡Œã”ã¨ã«æ–‡ç« ã‚’å‡¦ç†ã™ã‚‹
        sentences = re.split(r"(?<=ã€‚)", text)  # åŒã˜è¡Œã«ã‚ã‚‹è¤‡æ•°ã®æ–‡ç« ãŒã‚ã‚‹å ´åˆã¯åˆ†ã‘ã‚‹
        annotated_text_list, text_position_list, advice_list = wrapper_function(
            show_element,
            sentences,
            row_num,
            correspondence_dict,
            hankaku_zenkaku,
            tokenizer,
            overused_parts,
            problematic_level_dict,
        )
        annotated_texts += annotated_text_list
        pos_list += text_position_list
        advices_list += advice_list
        annotated_texts.append("  \n  \n")  # Streamlitã¯æ”¹è¡Œè¨˜å·ã®å‰ã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒ2ã¤å¿…è¦

    st.header("æœ¬æ–‡")
    annotated_text(*annotated_texts)

    with st.sidebar.expander(f"âœ…æŒ‡æ‘˜ç®‡æ‰€ï¼ˆ{len(pos_list)}ä»¶ï¼‰", expanded=True):
        if len(pos_list) == 0:
            st.write("æŒ‡æ‘˜ç®‡æ‰€ã¯ã‚ã‚Šã¾ã›ã‚“ğŸ¤“")
        if show_element in ["é•·ã™ãã‚‹æ–‡", "èª­ç‚¹ãŒå¤šã„æ–‡", "èª­ç‚¹ãŒãªã„æ–‡", "åŠè§’ãƒ»å…¨è§’"]:
            for item in pos_list:
                st.write(f"### {item[0]}  \n{item[1]}")
        elif show_element in ["å†—é•·ãªè¡¨ç¾", "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾"]:
            for item, advice in zip(pos_list, advices_list):
                st.write(f"### {item[0]}  \nä¿®æ­£æ¤œè¨è¡¨ç¾ï¼š {item[1]}  \n{advice}")
        else:
            conn = sqlite3.connect("./data/wnjpn.db")
            model = gensim.models.Word2Vec.load("./data/word2vec.gensim.model")
            overused_expressions_dict = {
                overused_expression: advices_list.count(overused_expression)
                for overused_expression in overused_parts
            }
            for (pos, expression), freq in overused_expressions_dict.items():
                ret_l = get_synonym.sort_word(model, conn, expression)
                cand = "ãƒ»".join(ret_l)
                if len(cand) == 0:
                    cand = "-"
                st.write(f"### ï¼ˆ{pos}ï¼‰{expression}ï¼š{freq}å›  \n  \né–¢é€£èªï¼š{cand}")

    with st.form(key="my_form", clear_on_submit=True):
        with st.sidebar:
            INPUT_LIMIT_LENGTH = 300
            long_sentence = st.text_input(
                label=f"ğŸ¤–ChatGPTãŒé•·ã„æ–‡ã‚’çŸ­ãã—ã¾ã™ã€‚ä»¥ä¸‹ã«{INPUT_LIMIT_LENGTH}å­—æœªæº€ã®æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚é€ä¿¡ã™ã‚‹åº¦ã«çµæœãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )
            style = st.radio(
                "âš ï¸é€ä¿¡å‰ã«å…ƒã®æ–‡ä½“ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚", ("å¸¸ä½“ï¼ˆã ãƒ»ã§ã‚ã‚‹èª¿ï¼‰", "æ•¬ä½“ï¼ˆã§ã™ãƒ»ã¾ã™èª¿ï¼‰"), horizontal=True
            )
            submit_button = st.form_submit_button(label="ChatGPTã«é€ä¿¡ï¼ˆ1åˆ†ã‚ãŸã‚Š3å›ã¾ã§ï¼‰")
            if submit_button:
                if len(long_sentence) >= INPUT_LIMIT_LENGTH:
                    st.write(
                        f"å…¥åŠ›ã™ã‚‹æ–‡ç« ã¯{INPUT_LIMIT_LENGTH}å­—æœªæº€ã«ã—ã¦ãã ã•ã„ã€‚ï¼ˆç¾åœ¨ã®æ–‡å­—æ•°ï¼š{len(long_sentence)}å­—ï¼‰"
                    )
                else:
                    well_written_text = proofread_with_chatgpt.proofreader(
                        long_sentence, style
                    )
                    st.write(f"æ ¡æ­£å‰ï¼š{long_sentence}")
                    st.write(f"æ ¡æ­£å¾Œï¼š{well_written_text}")
