import re
import argparse
import streamlit as st
from annotated_text import annotated_text
import sqlite3
import gensim

from module_length import length_funcs
from module_wordy import wordy_funcs
from module_expression import overused_funcs, preparation, get_synonym
from module_expression.config import POS_LIST


def create_layout():
    st.set_page_config(layout="wide")  # ãƒšãƒ¼ã‚¸ã®æ¨ªå¹…ã‚’ãƒ•ãƒ«ã«ä½¿ã†
    uploaded_file = st.sidebar.file_uploader("ğŸ“ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«", accept_multiple_files=False)
    show_element = st.sidebar.selectbox(
        "âš™ï¸è¡¨ç¤ºã™ã‚‹è¦ç´ ", ["é•·ã™ãã‚‹æ–‡", "èª­ç‚¹ãŒå¤šã„æ–‡", "èª­ç‚¹ãŒãªã„æ–‡", "å†—é•·ãªè¡¨ç¾", "ä½¿ã‚ã‚Œã™ããªè¡¨ç¾"]
    )

    selected_items = st.sidebar.multiselect(
        "ğŸ’¬å“è©ï¼ˆã€Œä½¿ã‚ã‚Œã™ããªè¡¨ç¾ã€ã‚’é¸ã‚“ã æ™‚ã®ã¿æœ‰åŠ¹ãƒ»è¤‡æ•°é¸æŠå¯ï¼‰",
        POS_LIST,
    )

    return uploaded_file, show_element, selected_items


def prepare_tools_for_analysis():
    wordy_expression_dict = wordy_funcs.create_wordy_expression_dict()
    tokenizer = preparation.prepare_tokenizer()
    return wordy_expression_dict, tokenizer


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--file_name", default="./data/demo.txt", help="put your text file name"
    )
    args = parser.parse_args()
    return args


def wrapper_function(
    show_element,
    sentences,
    row_num,
    wordy_expression_dict,
    tokenizer,
    overused_parts,
    problematic_level_dict,
):
    element_to_func = {
        "é•·ã™ãã‚‹æ–‡": length_funcs.lengthy_checker,
        "èª­ç‚¹ãŒå¤šã„æ–‡": length_funcs.punctuation_num_checker,
        "èª­ç‚¹ãŒãªã„æ–‡": length_funcs.continuous_checker,
        "å†—é•·ãªè¡¨ç¾": wordy_funcs.wordy_expression_checker,
        "ä½¿ã‚ã‚Œã™ããªè¡¨ç¾": overused_funcs.overused_expression_checker,
    }
    if show_element in ["é•·ã™ãã‚‹æ–‡", "èª­ç‚¹ãŒå¤šã„æ–‡", "èª­ç‚¹ãŒãªã„æ–‡"]:
        annotated_text_list, text_position_list, advice_list = element_to_func[
            show_element
        ](sentences, row_num)
    elif show_element == "å†—é•·ãªè¡¨ç¾":
        (
            annotated_text_list,
            text_position_list,
            advice_list,
        ) = element_to_func[
            show_element
        ](sentences, wordy_expression_dict, row_num)
    else:
        (
            annotated_text_list,
            text_position_list,
            advice_list,
        ) = element_to_func[
            show_element
        ](sentences, row_num, tokenizer, overused_parts, problematic_level_dict)
    return annotated_text_list, text_position_list, advice_list


if __name__ == "__main__":
    my_args = get_args()
    uploaded_file, show_element, selected_items = create_layout()
    if selected_items == []:
        selected_items = POS_LIST

    annotated_texts = []  # ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç¾¤
    pos_list = []  # æŒ‡æ‘˜ã—ãŸæ–‡ç« ã®å ´æ‰€
    advices_list = []  # æŒ‡æ‘˜ã®å…·ä½“çš„ãªå†…å®¹
    wordy_expression_dict, tokenizer = prepare_tools_for_analysis()
    f_r = open(my_args.file_name, "r")
    text_lists = (
        uploaded_file.read().decode("utf-8").splitlines()
        if uploaded_file is not None
        else f_r.read().splitlines()
    )
    f_r.close()
    overused_parts = overused_funcs.find_overused_part(  # é »ç™ºã™ã‚‹è¡¨ç¾ä¸Šä½20ä»¶ã®ãƒªã‚¹ãƒˆ
        text_lists, tokenizer, pos_option=selected_items
    )
    problematic_level_dict = overused_funcs.overused_level_indicator(overused_parts)
    for row_num, text in enumerate(text_lists):  # æ”¹è¡Œã”ã¨ã«æ–‡ç« ã‚’å‡¦ç†ã™ã‚‹
        sentences = re.split(r"(?<=ã€‚)", text)  # åŒã˜è¡Œã«ã‚ã‚‹è¤‡æ•°ã®æ–‡ç« ãŒã‚ã‚‹å ´åˆã¯åˆ†ã‘ã‚‹
        annotated_text_list, text_position_list, advice_list = wrapper_function(
            show_element,
            sentences,
            row_num,
            wordy_expression_dict,
            tokenizer,
            overused_parts,
            problematic_level_dict,
        )
        annotated_texts += annotated_text_list
        pos_list += text_position_list
        advices_list += advice_list
        annotated_texts.append("  \n  \n")  # Streamlitã¯æ”¹è¡Œè¨˜å·ã®å‰ã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒ2ã¤å¿…è¦

    st.header("æœ¬æ–‡")
    annotated_text(*annotated_texts)

    with st.sidebar.expander(f"âœ…æŒ‡æ‘˜ç®‡æ‰€ï¼ˆ{len(pos_list)}ä»¶ï¼‰", expanded=True):
        if len(pos_list) == 0:
            st.write("æŒ‡æ‘˜ç®‡æ‰€ã¯ã‚ã‚Šã¾ã›ã‚“ğŸ¤“")
        if show_element in ["é•·ã™ãã‚‹æ–‡", "èª­ç‚¹ãŒå¤šã„æ–‡", "èª­ç‚¹ãŒãªã„æ–‡"]:
            for item in pos_list:
                st.write(f"### {item[0]}  \n{item[1]}")
        elif show_element == "å†—é•·ãªè¡¨ç¾":
            for item, advice in zip(pos_list, advices_list):
                st.write(f"### {item[0]}  \nå†—é•·è¡¨ç¾ï¼š {item[1]}  \n{advice}")
        else:
            conn = sqlite3.connect("./data/wnjpn.db")
            model = gensim.models.Word2Vec.load("./data/word2vec.gensim.model")
            overused_expressions_dict = {
                overused_expression: advices_list.count(overused_expression)
                for overused_expression in overused_parts
            }
            for (pos, expression), freq in overused_expressions_dict.items():
                ret_l = get_synonym.sort_word(model, conn, expression)
                cand = "ãƒ»".join(ret_l)
                if len(cand) == 0:
                    cand = "-"
                st.write(f"### ï¼ˆ{pos}ï¼‰{expression}ï¼š{freq}å›  \n  \né–¢é€£èªï¼š{cand}")
