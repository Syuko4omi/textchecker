import re
import argparse
import streamlit as st
from annotated_text import annotated_text
import sqlite3
import gensim

from module_length import length_funcs, proofread_with_chatgpt
from module_wordy import wordy_funcs, tautological_funcs
from module_appearance import appearance_funcs
from module_expression import overused_funcs, preparation, get_synonym
from module_expression.config import POS_LIST


def create_layout():  # ã‚µã‚¤ãƒ‰ãƒãƒ¼ä»˜ãã®ãƒšãƒ¼ã‚¸ã®åŸºç¤éƒ¨åˆ†ã‚’ä½œã‚‹
    st.set_page_config(layout="wide")  # ãƒšãƒ¼ã‚¸ã®æ¨ªå¹…ã‚’ãƒ•ãƒ«ã«ä½¿ã†
    uploaded_file = st.sidebar.file_uploader("ğŸ“ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«", accept_multiple_files=False)

    show_element = st.sidebar.selectbox(
        "âš™ï¸è¡¨ç¤ºã™ã‚‹è¦ç´ ",
        [
            "é•·ã™ãã‚‹æ–‡",
            "èª­ç‚¹ãŒå¤šã„æ–‡",
            "èª­ç‚¹ãŒãªã„éƒ¨åˆ†",
            "å†—é•·ãªè¡¨ç¾",
            "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾",
            "åŠè§’ãƒ»å…¨è§’",
            "ä½¿ã‚ã‚Œã™ããªè¡¨ç¾",
        ],
    )

    hankaku_zenkaku = st.sidebar.selectbox(
        "ğŸ”¤è‹±æ•°å­—ãƒ»ã‚«ãƒŠãƒ»è¨˜å·ï¼ˆã€ŒåŠè§’ãƒ»å…¨è§’ã€ã‚’é¸ã‚“ã æ™‚ã®ã¿æœ‰åŠ¹ï¼‰",
        ["åŠè§’è‹±æ•°å­—", "å…¨è§’è‹±æ•°å­—", "åŠè§’ã‚«ã‚¿ã‚«ãƒŠ", "åŠè§’è¨˜å·"],
    )

    selected_items = st.sidebar.multiselect(
        "ğŸ’¬å“è©ï¼ˆã€Œä½¿ã‚ã‚Œã™ããªè¡¨ç¾ã€ã‚’é¸ã‚“ã æ™‚ã®ã¿æœ‰åŠ¹ãƒ»è¤‡æ•°é¸æŠå¯ï¼‰",
        POS_LIST,
    )
    st.header("æœ¬æ–‡")

    return uploaded_file, show_element, hankaku_zenkaku, selected_items


def prepare_tools_for_analysis():  # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒç”¨ã®è¾æ›¸ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ç”¨æ„ã™ã‚‹
    wordy_expression_dict = wordy_funcs.create_wordy_expression_dict()
    tautological_expression_dict = (
        tautological_funcs.create_tautological_expression_dict()
    )
    tokenizer = preparation.prepare_tokenizer()
    return wordy_expression_dict, tautological_expression_dict, tokenizer


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--file_name", default="./data/demo.txt", help="put your text file name"
    )
    args = parser.parse_args()
    return args


def wrapper_function(  # å¿…è¦ãªã‚‚ã®ã‚’å—ã‘å–ã‚Šã€å„æ©Ÿèƒ½ã«å‡¦ç†ã‚’æ¸¡ã™éƒ¨åˆ†
    show_element,
    sentences,
    row_num,
    correspondence_dict,
    hankaku_zenkaku,
    tokenizer,
    overused_parts,
    problematic_level_dict,
):
    element_to_func = {
        "é•·ã™ãã‚‹æ–‡": length_funcs.lengthy_checker,
        "èª­ç‚¹ãŒå¤šã„æ–‡": length_funcs.punctuation_num_checker,
        "èª­ç‚¹ãŒãªã„éƒ¨åˆ†": length_funcs.continuous_checker,
        "å†—é•·ãªè¡¨ç¾": wordy_funcs.wordy_expression_checker,
        "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾": tautological_funcs.tautological_expression_checker,
        "åŠè§’ãƒ»å…¨è§’": appearance_funcs.appearance_checker,
        "ä½¿ã‚ã‚Œã™ããªè¡¨ç¾": overused_funcs.overused_expression_checker,
    }
    if show_element in ["é•·ã™ãã‚‹æ–‡", "èª­ç‚¹ãŒå¤šã„æ–‡", "èª­ç‚¹ãŒãªã„éƒ¨åˆ†"]:  # æ–‡ç« ã®èª­ã¿ã‚„ã™ã•ã«é–¢ã™ã‚‹ã‚‚ã®
        annotated_text_list, text_position_list, advice_list = element_to_func[
            show_element
        ](sentences, row_num)
    elif show_element in ["å†—é•·ãªè¡¨ç¾", "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾"]:  # è¡¨ç¾ã«é–¢ã—ã¦æ”¹å–„æ¡ˆã‚’æç¤ºã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‚‚ã®
        annotated_text_list, text_position_list, advice_list = element_to_func[
            show_element
        ](sentences, correspondence_dict[show_element], row_num)
    elif show_element == "åŠè§’ãƒ»å…¨è§’":  # ä½“è£ã«é–¢ã‚ã‚‹ã‚‚ã®
        annotated_text_list, text_position_list, advice_list = element_to_func[
            show_element
        ](sentences, row_num, hankaku_zenkaku)
    else:  # é »å‡ºèªå½™ã«é–¢ã™ã‚‹ã‚‚ã®
        (
            annotated_text_list,
            text_position_list,
            advice_list,
        ) = element_to_func[
            show_element
        ](sentences, row_num, tokenizer, overused_parts, problematic_level_dict)
    return annotated_text_list, text_position_list, advice_list


def annotate_and_show_body_text(  # æ–‡ç« ã‚’åˆ†æã—ãŸçµæœã‚’è¿”ã—ã€è‰²ä»˜ãã®æ–‡ç« ã‚’è¡¨ç¤ºã™ã‚‹éƒ¨åˆ†
    file_name, uploaded_file, show_element, hankaku_zenkaku, selected_items
):
    annotated_texts = []  # ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç¾¤
    pos_list = []  # æŒ‡æ‘˜ã—ãŸæ–‡ç« ã®å ´æ‰€
    advices_list = []  # æŒ‡æ‘˜ã®å…·ä½“çš„ãªå†…å®¹
    (
        wordy_expression_dict,
        tautological_expression_dict,
        tokenizer,
    ) = prepare_tools_for_analysis()
    f_r = open(file_name, "r")
    text_lists = (
        uploaded_file.read().decode("utf-8").splitlines()
        if uploaded_file is not None
        else f_r.read().splitlines()
    )
    f_r.close()

    char_num = sum([len(item) for item in text_lists])
    if char_num > 70000:  # é•·ã™ãã‚‹æ–‡ç« ã¯ã€ãã®æ™‚ç‚¹ã§è§£æã‚’ä¸­æ­¢ã™ã‚‹
        st.write(f"å…¥åŠ›ã™ã‚‹æ–‡ç« ã¯æœ€å¤§ã§ã‚‚50000å­—ç¨‹åº¦ã«ã—ã¦ãã ã•ã„ã€‚ï¼ˆå…¥åŠ›ã•ã‚ŒãŸæ–‡å­—æ•°ï¼š{char_num}å­—ï¼‰")
        return pos_list, advices_list, []

    overused_parts = overused_funcs.find_overused_part(  # é »ç™ºã™ã‚‹è¡¨ç¾ä¸Šä½20ä»¶ã®ãƒªã‚¹ãƒˆ
        text_lists, tokenizer, pos_option=selected_items
    )
    problematic_level_dict = overused_funcs.overused_level_indicator(overused_parts)
    correspondence_dict = {
        "å†—é•·ãªè¡¨ç¾": wordy_expression_dict,
        "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾": tautological_expression_dict,
    }
    for row_num, text in enumerate(text_lists):  # æ”¹è¡Œã”ã¨ã«æ–‡ç« ã‚’å‡¦ç†ã™ã‚‹
        sentences = re.split(r"(?<=ã€‚)", text)  # åŒã˜è¡Œã«ã‚ã‚‹è¤‡æ•°ã®æ–‡ç« ãŒã‚ã‚‹å ´åˆã¯åˆ†ã‘ã‚‹
        annotated_text_list, text_position_list, advice_list = wrapper_function(
            show_element,
            sentences,
            row_num,
            correspondence_dict,
            hankaku_zenkaku,
            tokenizer,
            overused_parts,
            problematic_level_dict,
        )
        annotated_texts += annotated_text_list
        pos_list += text_position_list
        advices_list += advice_list
        annotated_texts.append("  \n  \n")  # Streamlitã¯æ”¹è¡Œè¨˜å·ã®å‰ã«åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ãŒ2ã¤å¿…è¦

    annotated_text(*annotated_texts)
    return pos_list, advices_list, overused_parts


if __name__ == "__main__":
    my_args = get_args()
    uploaded_file, show_element, hankaku_zenkaku, selected_items = create_layout()
    selected_items = POS_LIST if selected_items == [] else selected_items

    pos_list, advices_list, overused_parts = annotate_and_show_body_text(
        my_args.file_name, uploaded_file, show_element, hankaku_zenkaku, selected_items
    )

    # æŒ‡æ‘˜ã™ã‚‹ç®‡æ‰€ã¨ãã®å†…å®¹ã‚’è¡¨ç¤ºã™ã‚‹
    with st.sidebar.expander(f"âœ…æŒ‡æ‘˜ç®‡æ‰€ï¼ˆ{len(pos_list)}ä»¶ï¼‰", expanded=True):
        if len(pos_list) == 0:
            st.write("æŒ‡æ‘˜ç®‡æ‰€ã¯ã‚ã‚Šã¾ã›ã‚“ğŸ¤“")
        if show_element in ["é•·ã™ãã‚‹æ–‡", "èª­ç‚¹ãŒå¤šã„æ–‡", "èª­ç‚¹ãŒãªã„éƒ¨åˆ†", "åŠè§’ãƒ»å…¨è§’"]:
            for item in pos_list:
                st.write(f"### {item[0]}  \n{item[1]}")
        elif show_element in ["å†—é•·ãªè¡¨ç¾", "é‡è¤‡ã—ã¦ã„ã‚‹è¡¨ç¾"]:
            for item, advice in zip(pos_list, advices_list):
                st.write(f"### {item[0]}  \nä¿®æ­£æ¤œè¨è¡¨ç¾ï¼š {item[1]}  \n{advice}")
        else:
            conn = sqlite3.connect("./data/wnjpn.db")
            model = gensim.models.Word2Vec.load("./data/word2vec.gensim.model")
            overused_expressions_dict = {
                overused_expression: advices_list.count(overused_expression)
                for overused_expression in overused_parts
            }
            for (pos, expression), freq in overused_expressions_dict.items():
                ret_l = get_synonym.sort_word(model, conn, expression)
                cand = "ãƒ»".join(ret_l)
                if len(cand) == 0:
                    cand = "-"
                st.write(f"### ï¼ˆ{pos}ï¼‰{expression}ï¼š{freq}å›  \n  \né–¢é€£èªï¼š{cand}")

    # ChatGPTã«æ–‡ç« ã‚’æ¸¡ã™éƒ¨åˆ†
    with st.form(key="my_form", clear_on_submit=True):
        with st.sidebar:
            INPUT_LIMIT_LENGTH = 300
            long_sentence = st.text_input(
                label=f"ğŸ¤–ChatGPTãŒé•·ã„æ–‡ã‚’çŸ­ãã—ã¾ã™ã€‚ä»¥ä¸‹ã«{INPUT_LIMIT_LENGTH}å­—æœªæº€ã®æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚é€ä¿¡ã™ã‚‹åº¦ã«çµæœãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )
            style = st.radio(
                "âš ï¸é€ä¿¡å‰ã«å…ƒã®æ–‡ä½“ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚", ("å¸¸ä½“ï¼ˆã ãƒ»ã§ã‚ã‚‹èª¿ï¼‰", "æ•¬ä½“ï¼ˆã§ã™ãƒ»ã¾ã™èª¿ï¼‰"), horizontal=True
            )
            submit_button = st.form_submit_button(label="ChatGPTã«é€ä¿¡ï¼ˆ1åˆ†ã‚ãŸã‚Š3å›ã¾ã§ï¼‰")
            if submit_button:
                if len(long_sentence) >= INPUT_LIMIT_LENGTH:
                    st.write(
                        f"å…¥åŠ›ã™ã‚‹æ–‡ç« ã¯{INPUT_LIMIT_LENGTH}å­—æœªæº€ã«ã—ã¦ãã ã•ã„ã€‚ï¼ˆç¾åœ¨ã®æ–‡å­—æ•°ï¼š{len(long_sentence)}å­—ï¼‰"
                    )
                else:
                    well_written_text = proofread_with_chatgpt.proofreader(
                        long_sentence, style
                    )
                    st.write(f"æ ¡æ­£å‰ï¼š{long_sentence}")
                    st.write(f"æ ¡æ­£å¾Œï¼š{well_written_text}")
